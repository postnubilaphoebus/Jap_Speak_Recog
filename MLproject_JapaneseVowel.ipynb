{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Project: Japanese Vowel speaker classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data into time series arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data sets\n",
    "trainData = np.loadtxt(\"ae.train\")\n",
    "testData = np.loadtxt(\"ae.test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview: \n",
    "* Training: 270 (30 utterances by 9 speakers. See file 'size_ae.train'.) \n",
    "* Testing: 370 (24-88 utterances by the same 9 speakers in different opportunities. See file 'size_ae.test'.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining 270 training time series arrays\n",
    "# arrays are (N x 12); where N is length of time series recording and 12 is number of dimensions (ie channels)\n",
    "trainInputs = np.empty((270,1), dtype=object)\n",
    "readindex = 0\n",
    "\n",
    "for i in range(1,271):\n",
    "    readindex = readindex + 1  \n",
    "    l = 0\n",
    "    while trainData[readindex-1, 1] != 1:\n",
    "        l = l + 1 \n",
    "        readindex = readindex + 1\n",
    "    trainInputs[i-1,0] = trainData[readindex-l-1:readindex-1,:]\n",
    "\n",
    "\n",
    "# obtaining 370 test time series arrays \n",
    "# arrays are (N x 12); where N is length of time series recording and 12 is number of dimensions (ie channels)\n",
    "testInputs = np.empty((370,1), dtype=object)\n",
    "readindex = 0\n",
    "\n",
    "# The last 12 entries of each recording are 1s, indicating 12 channels\n",
    "# They are droppped when reading in the data\n",
    "for i in range(1,371):\n",
    "    readindex = readindex + 1\n",
    "    l = 0 \n",
    "    while testData[readindex-1, 1] != 1:\n",
    "        l = l+1 \n",
    "        readindex = readindex + 1\n",
    "    testInputs[i-1,0] = testData[readindex-l-1:readindex-1,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining 270 training outputs (speaker targets)\n",
    "# arrays are (N x 9); where N is length of time series recording and 9 is number of different speakers\n",
    "# the speaker is indicated with a '1'\n",
    "trainOutputs = np.empty((270,1), dtype=object)\n",
    "\n",
    "for i in range(1,271):\n",
    "    l = np.size(trainInputs[i-1,0],0)\n",
    "    teacher = np.zeros((l,9))\n",
    "    speakerIndex = np.ceil(i/30)\n",
    "    teacher[:,np.int(speakerIndex)-1] = 1 \n",
    "    trainOutputs[i-1,0] = teacher\n",
    "\n",
    "# obtaining 370 test outputs (speaker targets)\n",
    "# arrays are (N x 9); where N is length of time series recording and 9 is number of different speakers\n",
    "# the speaker is indicated with a '1'\n",
    "testOutputs = np.empty((370,1), dtype=object)\n",
    "speakerIndex = 1\n",
    "blockCounter = 0\n",
    "blockLengthes = [31, 35, 88, 44, 29, 24, 40, 50, 29]\n",
    "for i in range(1, 371):\n",
    "    blockCounter = blockCounter + 1 \n",
    "    if blockCounter == blockLengthes[speakerIndex-1] + 1:\n",
    "        speakerIndex = speakerIndex + 1\n",
    "        blockCounter = 1\n",
    "    l = np.size(testInputs[i-1,0], 0)\n",
    "    teacher = np.zeros((l,9))\n",
    "    teacher[:,np.int(speakerIndex)-1] = 1   \n",
    "    testOutputs[i-1, 0] = teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prints histogram of timeseries length (exploratory analysis)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "histos = np.zeros(270)\n",
    "\n",
    "for i in range(270):\n",
    "    histos[i] = (len(trainInputs[i, 0]))\n",
    "\n",
    "#print(histos)\n",
    "plt.title('Counting timeseries length (total 270)')\n",
    "plt.xlabel('Timeseries length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.hist(histos, bins = 20, ec='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add length of each recording as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RecordingLength = histos\n",
    "\n",
    "for i in range(270):\n",
    "    RecordingLength[i] = RecordingLength[i] / 26\n",
    "#print(RecordingLength)\n",
    "\n",
    "for i in range(270):\n",
    "    lenArray = len(trainInputs[i][0])\n",
    "    rec = np.full(lenArray, RecordingLength[i])\n",
    "    newArray = np.column_stack((trainInputs[i][0][:], rec))\n",
    "    trainInputs[i][0] = newArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing: Polynomial approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.8609360000000001 -0.20738300000000004 0.26155700000000004\n",
      "  -0.21456200000000003 -0.17125300000000004 -0.11816700000000001\n",
      "  -0.2775570000000001 0.025668000000000003 0.12670100000000004\n",
      "  -0.30675600000000003 -0.21307600000000002 0.08872800000000002 20.0]\n",
      " [1.8612997384809338 -0.18267564860375698 0.23066032694827415\n",
      "  -0.23210975711229836 -0.13138649129094818 -0.11807963739773934\n",
      "  -0.29651289668834213 -0.03313024304488419 0.16759635054539174\n",
      "  -0.28624022660364096 -0.24664031615731866 0.09767908741566142 20.0]\n",
      " [1.9502480607158543 -0.22399147883240778 0.25213937058999186\n",
      "  -0.28359093449549977 -0.07174701811840963 -0.10070745726830832\n",
      "  -0.35094318924536183 0.000719325107547616 0.17187577823353656\n",
      "  -0.3031130564842153 -0.24041328231675593 0.080195845212439 20.0]\n",
      " [1.8842704517550042 -0.2351111337998948 0.251752034317442\n",
      "  -0.27795948556258804 -0.029785404169083842 -0.11171942802410294\n",
      "  -0.396248012895325 0.016582010824505528 0.16950054042774218\n",
      "  -0.31793195814043296 -0.22070957990271942 0.07784953504439314 20.0]\n",
      " [1.7130983579040477 -0.2192950955944256 0.21539691395728913\n",
      "  -0.22678776514655447 -0.01844829393811928 -0.1378756069209547\n",
      "  -0.4025135335773288 -0.010690309329035468 0.163682164672883\n",
      "  -0.3239726586477804 -0.2100495969564155 0.09923195651347841 20.0]\n",
      " [1.7314249280057206 -0.26615401784592313 0.19731133168160359\n",
      "  -0.23263002557426346 -0.02534999243247862 -0.10771022868406052\n",
      "  -0.3721206628740858 -0.013944397233992298 0.13367041900021556\n",
      "  -0.34550826733501133 -0.21556342052428318 0.11313739555370503 20.0]\n",
      " [1.7222503714229869 -0.3024730029004788 0.1987768086204385\n",
      "  -0.2355025639944194 -0.05875678775255487 -0.03454276666458191\n",
      "  -0.3456412242686568 -0.010464941019076876 0.10073927990085454\n",
      "  -0.3612280633145803 -0.21274500100728075 0.1094109347545797 20.0]\n",
      " [1.6613127579568965 -0.3194681212997834 0.18543690000592405\n",
      "  -0.2325624820312022 -0.059975453482588856 0.0004297804082256537\n",
      "  -0.34546052317495324 -0.006611927805176887 0.07510191924002226\n",
      "  -0.36468578363901116 -0.19758094381790842 0.09572590323445322 20.0]\n",
      " [1.6372427144264183 -0.33795281306236696 0.14896925353424684\n",
      "  -0.220145630542181 -0.023857730872518686 -0.008779206466167883\n",
      "  -0.36643292292257257 -0.003205562413450751 0.053137136304624424\n",
      "  -0.3564803339969162 -0.179924686297179 0.08039963424403755 20.0]\n",
      " [1.645025094896001 -0.3489395073969589 0.12940736537769637\n",
      "  -0.1655986719427606 -0.02905590559641964 0.01662204083848652\n",
      "  -0.38235254245523287 -0.015190579524559187 0.02725180856994762\n",
      "  -0.3411111048460776 -0.1618262966401728 0.07612086948334867 20.0]\n",
      " [1.6186876370537817 -0.35649567239347196 0.1592419952922721\n",
      "  -0.12719635613471283 -0.06090256257065924 0.04582966352120835\n",
      "  -0.3660931745838022 -0.052117128427224796 -0.007511841299421324\n",
      "  -0.3497419714672917 -0.1305704986340611 0.11559953766828779 20.0]\n",
      " [1.6121727259151701 -0.43137620927069 0.1866417033800783\n",
      "  -0.10604935846083838 -0.03617233896700668 0.06763012085334404\n",
      "  -0.38088393024873984 -0.07547880180171185 -0.021773847071721933\n",
      "  -0.3545615334291703 -0.10703939957647082 0.12806206574244214 20.0]\n",
      " [1.6124443578387602 -0.5395473210195763 0.17681308566175616\n",
      "  -0.07778482934085658 0.043214955338058136 0.0832399610541244\n",
      "  -0.42143606116862353 -0.08399226913008717 -0.02478550850712659\n",
      "  -0.33317095350438286 -0.10327410466334065 0.0988217730930278 20.0]\n",
      " [1.52822560980121 -0.5544797444502367 0.14122752067142288\n",
      "  -0.035373481028176 0.09768627170754104 0.09095874798698515\n",
      "  -0.41766964293252623 -0.11505114698379182 -0.05240345841685954\n",
      "  -0.3097791758183739 -0.10128783113454687 0.10001429870731846 20.0]\n",
      " [1.415013278045999 -0.4932082336052964 0.07067197570723484\n",
      "  0.007639205067517199 0.1426071911270814 0.09692113895088202\n",
      "  -0.4197323738008143 -0.1471700106296107 -0.07039417127295829\n",
      "  -0.3191727701230801 -0.08321540745076506 0.12996336997365376 20.0]\n",
      " [1.3377754415094272 -0.47198491704479784 0.030729949947977713\n",
      "  0.021468587116525353 0.19096731239500914 0.08931583660351693\n",
      "  -0.4016464733869424 -0.17165874154106875 -0.09273643953856427\n",
      "  -0.3286653904442804 -0.06798924688162687 0.14247335770022748 20.0]\n",
      " [1.2870842102522464 -0.4989570850217172 0.028167159043831068\n",
      "  0.03253438909749009 0.22080503254128306 0.0728372155842927\n",
      "  -0.3709715079441952 -0.1981324980690762 -0.10096172671960951\n",
      "  -0.3198839300992148 -0.06643191076376162 0.12665655962532507 20.0]\n",
      " [1.2267508358209438 -0.5003924172483091 0.0022975687453795843\n",
      "  0.08823832292021716 0.22481440681303963 0.057937756787348374\n",
      "  -0.37302137046868156 -0.23046642947074075 -0.06806683452199305\n",
      "  -0.2998752815238291 -0.07417853298947104 0.09698232112385964 20.0]\n",
      " [1.191358080599268 -0.4990631488014661 -0.05618985185667458\n",
      "  0.15282890086832596 0.2671308266176773 0.025538846669432506\n",
      "  -0.39075240037094855 -0.2418440641539587 -0.05649092086542108\n",
      "  -0.25493844188353504 -0.09331146985973399 0.03632438130489454 20.0]\n",
      " [1.1695565749096686 -0.4884052010351833 -0.08081961550267755\n",
      "  0.1667627211748814 0.3308987836090376 -0.008620456824207295\n",
      "  -0.4151539404155857 -0.2064302094127446 -0.09786508928295026\n",
      "  -0.22035792074063684 -0.09027788675743098 -0.01635782820811072 20.0]\n",
      " [1.1628500281355003 -0.4836965929489183 -0.03299643732240584\n",
      "  0.15737997561087186 0.36416110852934885 -0.03669029771030622\n",
      "  -0.44999633839121794 -0.13263163463731437 -0.15861631275733978\n",
      "  -0.215406217989504 -0.05961137792221458 -0.049321116241989156 20.0]\n",
      " [1.1922748157138543 -0.50905948274685 0.06087730431111503\n",
      "  0.17492754872584187 0.35415732070689465 -0.06517643371061599\n",
      "  -0.4824001413223337 -0.06356078418146548 -0.1950915603956429\n",
      "  -0.22065215483241685 -0.02901281105163658 -0.083447249492301 20.0]\n",
      " [1.2507278840962306 -0.5493934507020736 0.14130415885367498\n",
      "  0.18410587400546963 0.3538934074442284 -0.0938144410061733\n",
      "  -0.47814953587412723 -0.04002736585301559 -0.21641301109882685\n",
      "  -0.20881663915770116 -0.019095805482469957 -0.12313751772930258 20.0]\n",
      " [1.2661025393338246 -0.5895862755850306 0.1892861089493474\n",
      "  0.19518972615778826 0.3624730195678916 -0.11889018708516572\n",
      "  -0.490256256210435 -0.02510728624407205 -0.21333942101408124\n",
      "  -0.20580798761314922 -0.020539638783475454 -0.1562318441218471 20.0]\n",
      " [1.244356919427913 -0.6209188611858475 0.21018095163404293\n",
      "  0.22082443522131864 0.37376410042683395 -0.1436368652376898\n",
      "  -0.5197830598745485 -0.0007689356314185183 -0.2010528251954516\n",
      "  -0.2165554814767765 -0.025458790694466576 -0.17455337049659125 20.0]\n",
      " [1.261441 -0.63835 0.217443 0.263384 0.391115 -0.176897 -0.510697\n",
      "  0.020526 -0.217709 -0.219071 -0.033314 -0.175986 20.0]]\n"
     ]
    }
   ],
   "source": [
    "# Flattening the input and doing POSTpadding -------\n",
    "# Easier to adapt in the poly approx code ----------\n",
    "# --------------------------------------------------\n",
    "\n",
    "size_max = 13 * 26\n",
    "Resample = np.empty((270,size_max), dtype=object)\n",
    "\n",
    "idx = 0 \n",
    "for element in trainInputs:\n",
    "    element = np.ndarray.flatten(element[0])\n",
    "    shape = np.shape(element)\n",
    "    padded_array = np.zeros(size_max)\n",
    "    padded_array[:shape[0]] = element  \n",
    "    element = padded_array\n",
    "    Resample[idx] = element\n",
    "    idx = idx + 1\n",
    "\n",
    "# Reshaping the input -------------------------------\n",
    "# ---------------------------------------------------\n",
    "Resample = Resample.reshape(270, 26, 13)\n",
    "\n",
    "# Replace padded zeros with polynomial approximations\n",
    "#----------------------------------------------------\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import interpolate\n",
    "\n",
    "def interpol(xValues, yValues, x):\n",
    "    cs = interpolate.splrep(xValues, yValues, k = 3)\n",
    "    return interpolate.splev(x, cs)\n",
    "\n",
    "for i in range(270):\n",
    "    for j in range(13):\n",
    "        if j == 12:\n",
    "            for m in range(26):\n",
    "                Resample[i][m][12] = Resample[i][0][12]\n",
    "        else:\n",
    "            recordingLength = round(Resample[i][0][12])\n",
    "            paddingLength = 26 - recordingLength\n",
    "            yValues = np.zeros(recordingLength)\n",
    "            xValues = np.zeros(recordingLength)\n",
    "            zValues = np.zeros(26)\n",
    "\n",
    "            for k in range(recordingLength):\n",
    "                xValues[k] = k + 1\n",
    "                yValues[k] = Resample[i][k][j]\n",
    "            for l in range(26):\n",
    "                zValues[l] = 1 + l * ((recordingLength - 1)/25)\n",
    "            #print(recordingLength)\n",
    "            #print(xValues)\n",
    "            #print(\"yValues: \")\n",
    "            #print(yValues)\n",
    "            #print(zValues)\n",
    "            predictedValues = interpol(xValues, yValues, zValues)\n",
    "            #print(\"predictValues: \")\n",
    "            #print(predictedValues)\n",
    "            for m in range(26):\n",
    "                Resample[i][m][j] = predictedValues[m]\n",
    "            \n",
    "#print(Resample[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise Resampling sample\n",
    "#---------------------------\n",
    "\n",
    "# Find Minimum value of each channel for each sample\n",
    "for i in range(270):\n",
    "    for j in range(12):\n",
    "        for k in range(26):\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing: Subtract minimum value of channel from each value. Finish with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#according to the paper by Dwarampudi et al 2019(in whatsappchat), prepadding is preferred. \n",
    "#However we seem to get better results with postpadding. That's why both are kept for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First: Find minimum of all channels of all samples\n",
    "minimum = np.zeros(13)\n",
    "for sample in trainInputs:\n",
    "    for array in sample[0]:\n",
    "        channelnumber = 0\n",
    "        for number in array:\n",
    "            if number < minimum[channelnumber]:\n",
    "                minimum[channelnumber] = number\n",
    "            channelnumber = channelnumber + 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Subtract minimum value from all values per channel\n",
    "trainInputsNonNegative = copy.deepcopy(trainInputs)\n",
    "samplenumber = 0\n",
    "for sample in trainInputsNonNegative:\n",
    "    arraynumber = 0\n",
    "    for array in sample[0]:\n",
    "        channelnumber = 0\n",
    "        for number in array:\n",
    "            trainInputsNonNegative[samplenumber][0][arraynumber][channelnumber] = number - minimum[channelnumber]\n",
    "            channelnumber = channelnumber + 1\n",
    "        arraynumber = arraynumber + 1\n",
    "    samplenumber = samplenumber + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pads the training inputs with zeroes to make all the timeseries of equal length\n",
    "size_max = 13 * 26\n",
    "trainInputsNonNegativePre = np.empty((270,size_max), dtype=object)\n",
    "trainInputsNonNegativePost = np.empty((270,size_max), dtype=object)\n",
    "\n",
    "idx = 0 \n",
    "for element in trainInputsNonNegative:\n",
    "    element = np.ndarray.flatten(element[0])\n",
    "    # Pads zeroes before    \n",
    "    elements = element\n",
    "    elements = np.pad(elements, (size_max - len(elements), 0), 'constant')\n",
    "    trainInputsNonNegativePre[idx] = elements\n",
    "    \n",
    "    # Pads zeroes after\n",
    "    # Pad element with zeroes until it reaches the shape of the largest timeseries (12 * 26 = 312)\n",
    "    shape = np.shape(element)\n",
    "    padded_array = np.zeros(size_max)\n",
    "    padded_array[:shape[0]] = element  \n",
    "    element = padded_array\n",
    "    # print(element)\n",
    "\n",
    "    trainInputsNonNegativePost[idx] = element\n",
    "    idx = idx + 1\n",
    "\n",
    "trainOutputsNew = np.empty((270,1), dtype=object)\n",
    "\n",
    "# Transforms the trainOutputs in classes 1-9\n",
    "idxx = 0\n",
    "for elements in trainOutputs:\n",
    "    for i in range(len(elements[0][0])):\n",
    "       if elements[0][0][i] == 1:\n",
    "           trainOutputsNew[idxx] = i + 1\n",
    "           idxx = idxx + 1\n",
    "        \n",
    "trainOutputsNew = np.ravel(trainOutputsNew)\n",
    "trainOutputsNew = trainOutputsNew.astype('int')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing 2: Put numbers in range 0-1. Finish with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additionally set numbers between 0/1\n",
    "#Find maximum per channel\n",
    "trainInputsZeroOne = copy.deepcopy(trainInputsNonNegative)\n",
    "maximum = np.zeros(13)\n",
    "for sample in trainInputsNonNegative:\n",
    "    for array in sample[0]:\n",
    "        channelnumber = 0\n",
    "        for number in array:\n",
    "            if number > maximum[channelnumber]:\n",
    "                maximum[channelnumber] = number\n",
    "            channelnumber = channelnumber + 1\n",
    "\n",
    "#divide by maximum value per channel\n",
    "samplenumber = 0\n",
    "for sample in trainInputsZeroOne:\n",
    "    arraynumber = 0\n",
    "    for array in sample[0]:\n",
    "        channelnumber = 0\n",
    "        for number in array:\n",
    "            trainInputsZeroOne[samplenumber][0][arraynumber][channelnumber] = number / maximum[channelnumber]\n",
    "            channelnumber = channelnumber + 1\n",
    "        arraynumber = arraynumber + 1\n",
    "    samplenumber = samplenumber + 1\n",
    "\n",
    "#pad the samples\n",
    "size_max = 13 * 26\n",
    "trainInputsZeroOnePre = np.empty((270,size_max), dtype=object)\n",
    "trainInputsZeroOnePost = np.empty((270,size_max), dtype=object)\n",
    "\n",
    "idx = 0 \n",
    "for element in trainInputsZeroOne:\n",
    "    element = np.ndarray.flatten(element[0])\n",
    "    # Pads zeroes before    \n",
    "    elements = element\n",
    "    elements = np.pad(elements, (size_max - len(elements), 0), 'constant')\n",
    "    trainInputsZeroOnePre[idx] = elements\n",
    "    \n",
    "    # Pads zeroes after\n",
    "    # Pad element with zeroes until it reaches the shape of the largest timeseries (13 * 26 = 338)\n",
    "    shape = np.shape(element)\n",
    "    padded_array = np.zeros(size_max)\n",
    "    padded_array[:shape[0]] = element  \n",
    "    element = padded_array\n",
    "\n",
    "    trainInputsZeroOnePost[idx] = element\n",
    "    idx = idx + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation of the Data using a cubic polynomial\n",
    "#### (in other words: approximating the data with a simple 1D polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "#trainInputsZeroOnePre\n",
    "\n",
    "def interpol(xValues, yValues, x):\n",
    "    cs = interpolate.splrep(xValues, yValues)\n",
    "    return interpolate.splev(x, cs)\n",
    "\n",
    "samplingFactor = 1\n",
    "sizePoly = 26 * samplingFactor\n",
    "\n",
    "inputReshaped = trainInputsZeroOnePre.reshape(270, 26, 13)\n",
    "polyAugment = np.zeros((270, sizePoly, 13))\n",
    "NoPaddingButResampling = inputReshaped\n",
    "\n",
    "for i in range(270):\n",
    "    for j in range(13):\n",
    "        if j == 12:\n",
    "            num = NoPaddingButResampling[i][25][12]\n",
    "            for m in range(26):\n",
    "                NoPaddingButResampling[i][m][12] = num\n",
    "\n",
    "            # code below doesn't work (commented out)\n",
    "            # recordingLength is from the previous iteration and you're actually not assigning anything\n",
    "\n",
    "            #finalInput = np.zeros(samplingFactor * recordingLength)\n",
    "            #for m in range (samplingFactor * recordingLength):\n",
    "                #finalInput[m] = inputReshaped[i][25][12]\n",
    "\n",
    "        recordingLength = round(inputReshaped[i][25][12] * 26)\n",
    "        paddingLength = 26 - recordingLength\n",
    "        yValues = np.zeros(recordingLength)\n",
    "        xValues = np.zeros(recordingLength)\n",
    "\n",
    "        for idx in range(recordingLength):\n",
    "            xValues[idx] = idx\n",
    "\n",
    "        for k in range(26):\n",
    "            if k >= paddingLength:\n",
    "                yValues[k - paddingLength] = inputReshaped[i][k][j]\n",
    "        \n",
    "        zValues = np.zeros(samplingFactor * recordingLength * 3)\n",
    "        for idx in range(1, samplingFactor * recordingLength * 3):\n",
    "            zValues[idx] = zValues[idx - 1] + (1 / samplingFactor)\n",
    "     \n",
    "        augmentedArray = interpol(xValues, yValues, zValues)\n",
    "\n",
    "        for s in range(paddingLength):\n",
    "            NoPaddingButResampling[i][s][j] = augmentedArray[s]\n",
    "\n",
    "        for idx in range(samplingFactor * recordingLength):\n",
    "            polyAugment[i][idx + samplingFactor * paddingLength][j] = augmentedArray[idx]\n",
    "        \n",
    "\n",
    "#print(polyAugment)\n",
    "#print(NoPaddingButResampling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing 3: Induce bias by squaring. Finish with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize/Bias numbers by squaring\n",
    "trainInputsSquared = copy.deepcopy(trainInputsZeroOne)\n",
    "\n",
    "#divide by maximum value per channel\n",
    "samplenumber = 0\n",
    "for sample in trainInputsSquared:\n",
    "    arraynumber = 0\n",
    "    for array in sample[0]:\n",
    "        channelnumber = 0\n",
    "        for number in array:\n",
    "            trainInputsSquared[samplenumber][0][arraynumber][channelnumber] = number *number\n",
    "            channelnumber = channelnumber + 1\n",
    "        arraynumber = arraynumber + 1\n",
    "    samplenumber = samplenumber + 1\n",
    "\n",
    "#pad normalized further samples\n",
    "size_max = 13 * 26\n",
    "trainInputsSquaredPre = np.empty((270,size_max), dtype=object)\n",
    "trainInputsSquaredPost = np.empty((270,size_max), dtype=object)\n",
    "\n",
    "idx = 0 \n",
    "for element in trainInputsSquared:\n",
    "    element = np.ndarray.flatten(element[0])\n",
    "    # Pads zeroes before    \n",
    "    elements = element\n",
    "    elements = np.pad(elements, (size_max - len(elements), 0), 'constant')\n",
    "    trainInputsSquaredPre[idx] = elements\n",
    "    \n",
    "    # Pads zeroes after\n",
    "    # Pad element with zeroes until it reaches the shape of the largest timeseries (13 * 26 = 338)\n",
    "    shape = np.shape(element)\n",
    "    padded_array = np.zeros(size_max)\n",
    "    padded_array[:shape[0]] = element  \n",
    "    element = padded_array\n",
    "\n",
    "    trainInputsSquaredPost[idx] = element\n",
    "    idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Crossvalidation. Currently only splitting in train-test data. Ideally, we want a validation set as well (e.g. 80 - 10 - 10 or 60 - 20 - 20)\n",
    "# Function taken from my Intro to Data Science assignment 3 code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def splitData(inputs, outputs):   \n",
    "    # To avoid overfitting, we divide the dataset into a part for training and a part for testing\n",
    "    # We split the dataset into 80% training data and 20% testing data\n",
    "    inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(\n",
    "            inputs, outputs, test_size=0.20) \n",
    "    \n",
    "    return inputs_train, inputs_test, outputs_train, outputs_test\n",
    "\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = splitData(trainInputsNonNegative, trainOutputsNew)\n",
    "\n",
    "print('Length inputs_train:', len(inputs_train))\n",
    "print('Length outputs_train:', len(outputs_train))\n",
    "print('Length inputs_test:', len(trainOutputsNew))\n",
    "print('Length trainOutputsNew:', len(trainOutputsNew))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Implementation Support Vector Machine\n",
    "def SVM(inputs_train, outputs_train, inputs_test):    \n",
    "    # Create a classifier \n",
    "    classifier = svm.SVC(kernel='linear')    \n",
    "    outputs_train = outputs_train.astype('int')\n",
    "    classifier.fit(inputs_train, outputs_train)\n",
    "    \n",
    "    # Predict the test data\n",
    "    labels_prediction = classifier.predict(inputs_test)\n",
    "\n",
    "    return labels_prediction\n",
    "\n",
    "def predictLabels(trainInputs, trainOutputs):\n",
    "    inputs_train, inputs_test, outputs_train, outputs_test = splitData(trainInputs, trainOutputs)\n",
    "\n",
    "    # Predict the test labels\n",
    "    prediction = SVM(inputs_train, outputs_train, inputs_test)\n",
    "\n",
    "    # Print results\n",
    "    wrong = 0\n",
    "    length = len(prediction)\n",
    "    for i in range(length):\n",
    "        #print(prediction[i], np.ravel(outputs_test)[i])\n",
    "        if(prediction[i] != np.ravel(outputs_test)[i]):\n",
    "            wrong = wrong + 1\n",
    "\n",
    "    return ((length - wrong) / length) * 100\n",
    "\n",
    "#Pre is for prepadded, Post for postpadded\n",
    "accuracy_NonNegativePre = np.zeros(10)\n",
    "accuracy_NonNegativePost = np.zeros(10)\n",
    "accuracy_ZeroOnePre = np.zeros(10)\n",
    "accuracy_ZeroOnePost = np.zeros(10)\n",
    "accuracy_SquaredPre = np.zeros(10)\n",
    "accuracy_SquaredPost = np.zeros(10)\n",
    "for j in range(10):\n",
    "    accuracy_NonNegativePre[j] = predictLabels(trainInputsNonNegativePre, trainOutputsNew)\n",
    "    accuracy_NonNegativePost[j] = predictLabels(trainInputsNonNegativePost, trainOutputsNew)\n",
    "    accuracy_ZeroOnePre[j] = predictLabels(trainInputsZeroOnePre, trainOutputsNew)\n",
    "    accuracy_ZeroOnePost[j] = predictLabels(trainInputsZeroOnePost, trainOutputsNew)\n",
    "    accuracy_SquaredPre[j] = predictLabels(trainInputsSquaredPre, trainOutputsNew)\n",
    "    accuracy_SquaredPost[j] = predictLabels(trainInputsSquaredPost, trainOutputsNew)\n",
    "    \n",
    "print(\"Average accuracy NonNegativePre:\", np.mean(accuracy_NonNegativePre))\n",
    "print(\"Average accuracy NonNegativePost:\", np.mean(accuracy_NonNegativePost))\n",
    "print(\"Average accuracy ZeroOnePre:\", np.mean(accuracy_ZeroOnePre))\n",
    "print(\"Average accuracy ZeroOnePost:\", np.mean(accuracy_ZeroOnePost))\n",
    "print(\"Average accuracy SquaredPre:\", np.mean(accuracy_SquaredPre))\n",
    "print(\"Average accuracy SquaredPost:\", np.mean(accuracy_SquaredPost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform preprocessing: PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try: all data in one vector\n",
    "trainInputsList = [trainInputsNonNegativePre, trainInputsNonNegativePost, trainInputsZeroOnePre, trainInputsZeroOnePost, trainInputsSquaredPre, trainInputsSquaredPost]\n",
    "Outputs = np.reshape(trainOutputsNew, (270,1))\n",
    "for PCAInputs in trainInputsList:\n",
    "    #print(Outputs)\n",
    "    allTrainInputs = np.concatenate([PCAInputs,Outputs],axis=1)\n",
    "    \n",
    "    datasetPCA = pd.DataFrame(allTrainInputs)\n",
    "    \n",
    "    pca = PCA(n_components=5)\n",
    "    pca.fit(datasetPCA)\n",
    "    principalComponents = pca.fit_transform(datasetPCA)\n",
    "\n",
    "    print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# I used the following code to optimise the parameters of SOM:\n",
    "#from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "#param_grid = {\n",
    "    #\"n_rows\": [5, 10, 20, 40, 60, 80, 100],\n",
    "    #\"n_columns\": [5, 20, 40, 40, 60, 80, 100],\n",
    "    #\"learning_rate_start\": [0.3, 0.5, 0.7, 0.9],\n",
    "    #\"learning_rate_end\": [0.1, 0.05, 0.005],\n",
    "#}\n",
    "\n",
    "#som = susi.SOMRegressor()\n",
    "#clf = RandomizedSearchCV(som, param_grid, random_state=1)\n",
    "#clf.fit(trainInputsSquaredPost, trainOutputsNew)\n",
    "#print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for the best parameters:\n",
    "# {'n_rows': 40, 'n_columns': 40, 'learning_rate_start': 0.7, 'learning_rate_end': 0.005}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reduction with Self-organising maps (SOM)\n",
    "# pip3 install susi\n",
    "import sys\n",
    "sys.path.append('/Users/lauridsstockert/opt/anaconda3/lib/python3.7/site-packages')\n",
    "\n",
    "import susi\n",
    "\n",
    "som = susi.SOMClustering(n_rows=40, n_columns = 40, learning_rate_start=0.7, learning_rate_end=0.005)\n",
    "\n",
    "inputs_train_som = som.fit_transform(trainInputsSquaredPost)\n",
    "inputs_train_som2 = som.fit_transform(trainInputsSquaredPre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy is generally around 50-60% It should be expected that it is lower than before, because no sophisticated algorithm has been applied (only SVM)\n",
    "accuracy_input_squared_padded_after = np.zeros(100)\n",
    "accuracy_input_squared_padded_before = np.zeros(100)\n",
    "for i in range(100):\n",
    "    accuracy_input_squared_padded_after[i] = predictLabels(inputs_train_som, trainOutputsNew)\n",
    "    accuracy_input_squared_padded_before[i] = predictLabels(inputs_train_som2, trainOutputsNew)\n",
    "print(\"Av. Accuracy (padding after, squared, and normalised)\", np.mean(accuracy_input_squared_padded_after))\n",
    "print(\"Av. Accuracy (padding before, squared, and normalised)\", np.mean(accuracy_input_squared_padded_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reduction & visualisation with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reduction with UMAP\n",
    "# pip3 install umap-learn\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "# Installing didn't work on my machine, so I provided tha path manually\n",
    "sys.path.append('/Users/lauridsstockert/opt/anaconda3/lib/python3.7/site-packages')\n",
    "\n",
    "import umap\n",
    "\n",
    "sns.set(style='white', context='poster', rc={'figure.figsize':(14,10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umapCalc(input_data, n_neighbors, n_components, min_dist):\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, n_components = n_components, min_dist = min_dist)\n",
    "    embedding = reducer.fit_transform(input_data)\n",
    "    return embedding\n",
    "\n",
    "def drawUmap(input_data, input_labels, n_neighbors=15, min_dist=0.1, n_components=2, metric='euclidean', title=''):\n",
    "    fit = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_components,\n",
    "        metric=metric\n",
    "    )\n",
    "    u = fit.fit_transform(input_data)\n",
    "    fig = plt.figure()\n",
    "    if n_components == 1:\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(u[:,0], range(len(u)), c=input_labels)\n",
    "    if n_components == 2:\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(u[:,0], u[:,1], c=input_labels)\n",
    "    if n_components == 3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(u[:,0], u[:,1], u[:,2], c=input_labels, s=100)\n",
    "    if n_components > 3:\n",
    "        print(\"ERROR: Cannot draw more than 3 dimensions\")\n",
    "    plt.title(title, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying Data Reduction with UMAP\n",
    "# Results range can get as high as 77% (best try so far)\n",
    "# I have commented out the code for parameter optimisation\n",
    "\n",
    "#accuracy_padded_after = np.zeros(100)\n",
    "######accuracy_padded_before = np.zeros(100)\n",
    "#cnt = 1\n",
    "\n",
    "#for n in (2, 5, 10, 20):\n",
    "    #for d in (0.1, 0.2, 0.4):\n",
    "        #for c in (2, 5):\n",
    "            #accuracy_padded_after = np.zeros(100)\n",
    "            #inputs_train_umap = umapCalc(trainInputsSquaredPost, n, c, d)\n",
    "            #for i in range(100):\n",
    "                #accuracy_padded_after[i] = predictLabels(inputs_train_umap, trainOutputsNew)\n",
    "            #print(\"Params are n_neighbors(n), min_dist(d), and n_components(c) \\n  n = {}, d = {}, c = {}\".format(n, d, c))\n",
    "            #print(\"Av. Accuracy (padding after)\", np.mean(accuracy_padded_after))\n",
    "            #print(\"\\n\")\n",
    "            #cnt = cnt + 1\n",
    "# empirically good combinations:\n",
    "# Acc = 77%: n = 50, d = 0.99, c = 7\n",
    "# Acc = 63.37%: n = 20, d = 0.8, c = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in (2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12):\n",
    "    drawUmap(input_data = trainInputsSquaredPost, input_labels = trainOutputsNew, n_neighbors=n, n_components = 3, title='n_neighbors = {}'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in (0.0, 0.1, 0.25, 0.5, 0.8, 0.99):\n",
    "    drawUmap(input_data = trainInputsSquaredPost, input_labels = trainOutputsNew, min_dist=d, title='min_dist = {}'.format(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(trainInputsNonNegativePre)\n",
    "\n",
    "# Prints PCA plot (Does only work with a PCA with 2 components)\n",
    "# Code based on https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "principalComponents = pca.fit_transform(trainInputsNonNegativePre)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "finalDf = pd.concat([principalDf, pd.DataFrame(Outputs)], axis = 1)\n",
    "#print(finalDf)\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "fig = plt.figure(figsize = (12,12))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "# Choose the speakers that you want to visualize in the plot\n",
    "targets = [1, 3, 4]\n",
    "colors = 'r', 'g', 'b'\n",
    "# Uncomment if you want to visualize all speakers\n",
    "#targets = [1,2,3,4,5,6,7,8,9]\n",
    "#colors = ['r', 'g', 'b', 'k', 'c', 'm', 'y', 'tab:orange', 'tab:brown']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf[0] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM classifier implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for those packages install tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predictLabelsLSTM(trainInputs, trainOutputs):\n",
    "    inputs_train, inputs_test, outputs_train, outputs_test = splitData(trainInputs, trainOutputs)\n",
    "    \n",
    "    inputs_train = np.asarray(inputs_train).astype('float32')\n",
    "    inputs_test = np.asarray(inputs_test).astype('float32')\n",
    "    outputs_train = np.asarray(outputs_train).astype('float32')\n",
    "    outputs_test = np.asarray(outputs_test).astype('float32')\n",
    "    #choose embedding length that is equal to the amount of dimensions\n",
    "    #embedding_vecor_length = 13\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units = 100, return_sequences = True, input_shape = inputs_train[0])))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Bidirectional(LSTM(units = 100)))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='Adamax', metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    #[print(i.shape, i.dtype) for i in model.inputs]\n",
    "    #[print(o.shape, o.dtype) for o in model.outputs]\n",
    "    #[print(l.name, l.input_shape, l.dtype) for l in model.layers]\n",
    "    \n",
    "    model.fit(inputs_train, outputs_train, epochs=17, batch_size=4)\n",
    "    \n",
    "    scores = model.evaluate(inputs_test, outputs_test, verbose=0)\n",
    "\n",
    "    return scores[1]*100\n",
    "\n",
    "#Pre is for prepadded, Post for postpadded\n",
    "accuracy_NonNegativePre = np.zeros(1)\n",
    "accuracy_NonNegativePost = np.zeros(1)\n",
    "\n",
    "\n",
    "#--------------\n",
    "accuracy_ZeroOnePre = np.zeros(1)\n",
    "#--------------\n",
    "accuracy_PolyAugment = np.zeros(1)\n",
    "\n",
    "accuracy_NoPaddingButResampling = np.zeros(1)\n",
    "\n",
    "\n",
    "\n",
    "accuracy_ZeroOnePost = np.zeros(1)\n",
    "accuracy_SquaredPre = np.zeros(1)\n",
    "accuracy_SquaredPost = np.zeros(1)\n",
    "\n",
    "#get Input into LSTM readable format:\n",
    "frames = 26\n",
    "dimensions = 13\n",
    "#LSTMNonNegativePre = trainInputsNonNegativePre.reshape(270, 26, 13)\n",
    "#LSTMNonNegativePost = trainInputsNonNegativePost.reshape(270, 26, 13)\n",
    "LSTMZeroOnePre = trainInputsZeroOnePre.reshape(270, 26, 13)\n",
    "#LSTMZeroOnePost = trainInputsZeroOnePost.reshape(270, 26, 13)\n",
    "#LSTMSquaredPre = trainInputsSquaredPre.reshape(270, 26, 13)\n",
    "#LSTMSquaredPost = trainInputsSquaredPost.reshape(270, 26, 13)\n",
    "\n",
    "for j in range(1):\n",
    "    #accuracy_NonNegativePre[j] = predictLabelsLSTM(LSTMNonNegativePre, trainOutputsNew)\n",
    "    #accuracy_NonNegativePost[j] = predictLabelsLSTM(LSTMNonNegativePost, trainOutputsNew)\n",
    "    accuracy_ZeroOnePre[j] = predictLabelsLSTM(LSTMZeroOnePre, trainOutputsNew)\n",
    "    #accuracy_PolyAugment[j] = predictLabelsLSTM(polyAugment, trainOutputsNew)\n",
    "    #accuracy_NoPaddingButResampling[j] = predictLabelsLSTM(NoPaddingButResampling, trainOutputsNew)\n",
    "    #accuracy_ZeroOnePost[j] = predictLabelsLSTM(LSTMZeroOnePost, trainOutputsNew)\n",
    "    #accuracy_SquaredPre[j] = predictLabelsLSTM(LSTMSquaredPre, trainOutputsNew)\n",
    "    #accuracy_SquaredPost[j] = predictLabelsLSTM(LSTMSquaredPost, trainOutputsNew)\n",
    "    \n",
    "#print(\"Average accuracy NonNegativePre:\", np.mean(accuracy_NonNegativePre))\n",
    "#print(\"Average accuracy NonNegativePost:\", np.mean(accuracy_NonNegativePost))\n",
    "print(\"Average accuracy ZeroOnePre:\", np.mean(accuracy_ZeroOnePre))\n",
    "#print(\"Average accuracy PolyAugment:\", np.mean(accuracy_PolyAugment))\n",
    "#print(\"Average accuracy ZeroOnePost:\", np.mean(accuracy_ZeroOnePost))\n",
    "#print(\"Average accuracy SquaredPre:\", np.mean(accuracy_SquaredPre))\n",
    "#print(\"Average accuracy SquaredPost:\", np.mean(accuracy_SquaredPost))\n",
    "#print(\"Average accuracy NoPaddingButResampling\", np.mean(accuracy_NoPaddingButResampling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECHO classifier implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from pyESN import ESN\n",
    "\n",
    "esn = ESN(n_inputs = 12,\n",
    "          n_outputs = 9,\n",
    "          n_reservoir = 2,\n",
    "          spectral_radius = 1.5,\n",
    "          random_state=42)\n",
    "\n",
    "train_error = []\n",
    "\n",
    "for i in range(270):\n",
    "    sample = trainInputs[i][0]      # state vector\n",
    "    output = trainOutputs[i][0]\n",
    "    train_pred, train_error_sample = esn.fit(sample, output)\n",
    "    train_error.append(train_error_sample)\n",
    "\n",
    "x = esn.fit(trainInputs, trainOutputsNew)\n",
    "    \n",
    "test_error = []\n",
    "\n",
    "for i in range(370):\n",
    "    test_pred = esn.predict(testInputs[i][0])\n",
    "    test_error_sample = np.sqrt(np.mean((test_pred - testOutputs[i][0])**2))\n",
    "    test_error.append(test_error_sample)\n",
    "    \n",
    "print(\"Mean training error (RMSE):\")   # root-mean-square-error\n",
    "print(np.mean(train_error))\n",
    "print(\"Mean test error (RMSE):\")\n",
    "print(np.mean(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter optimization + cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esn = ESN(n_inputs = 12,\n",
    "          n_outputs = 9,\n",
    "          random_state=42)\n",
    "\n",
    "# parameter optimization\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=269, n_repeats=2, random_state=42)\n",
    "param_grid = {\"n_reservoir\": [200, 400, 600, 800, 1000]}\n",
    "\n",
    "clf = GridSearchCV(esn, param_grid)\n",
    "\n",
    "m_scores = cross_val_score(esn, trainInputs, trainOutputs, cv=cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}